{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-resnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K432KekrkbMt",
        "outputId": "e1fd398b-4fbf-4001-a0ce-6dfef68f01ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-resnet\n",
            "  Downloading keras-resnet-0.2.0.tar.gz (9.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: keras>=2.2.4 in /usr/local/lib/python3.10/dist-packages (from keras-resnet) (3.5.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.4->keras-resnet) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.4->keras-resnet) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.4->keras-resnet) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.4->keras-resnet) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.4->keras-resnet) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.4->keras-resnet) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.4->keras-resnet) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.4->keras-resnet) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras>=2.2.4->keras-resnet) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=2.2.4->keras-resnet) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=2.2.4->keras-resnet) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.2.4->keras-resnet) (0.1.2)\n",
            "Building wheels for collected packages: keras-resnet\n",
            "  Building wheel for keras-resnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-resnet: filename=keras_resnet-0.2.0-py2.py3-none-any.whl size=20452 sha256=953de13a6344a183aefa0f30e542b46ce9167878b7684992d4c1f0dc1dc50e25\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/af/88/a668b279c5eadbe55dcaf6207f09059135166cefb09088bacc\n",
            "Successfully built keras-resnet\n",
            "Installing collected packages: keras-resnet\n",
            "Successfully installed keras-resnet-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "paWR0nlfF-CQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras_resnet.models import ResNet18\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tensorflow.image import resize\n",
        "import tensorflow as tf\n",
        "import copy\n",
        "import pickle\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mGpiFm8xY3IM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c813bd6-2273-42e0-ef03-463644d1f544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_96_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Initialize the MobileNetV2 model for feature extraction\n",
        "# Input shape is set to (96, 96, 3) for a balance between speed and feature quality\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "feature_extractor = MobileNetV2(weights='imagenet', include_top=False, pooling='avg', input_shape=(96, 96, 3))\n",
        "\n",
        "def extract_features(data):\n",
        "    \"\"\"\n",
        "    Extracts features from image data using the pre-trained MobileNetV2 model.\n",
        "\n",
        "    Args:\n",
        "        data (numpy.ndarray): Array of image data with shape (num_samples, 32, 32, 3).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Extracted feature vectors.\n",
        "    \"\"\"\n",
        "    # Resize images from (32, 32) to (96, 96) as MobileNetV2 expects larger images\n",
        "    resized_data = tf.image.resize(data, (96, 96)).numpy()\n",
        "    preprocessed_data = preprocess_input(resized_data)\n",
        "\n",
        "    # Extract features in batches to optimize memory usage\n",
        "    batch_size = 64\n",
        "    num_samples = preprocessed_data.shape[0]\n",
        "    features = []\n",
        "\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "        batch = preprocessed_data[i:i+batch_size]\n",
        "        batch_features = feature_extractor.predict(batch, verbose=0)\n",
        "        # Normalize features for cosine similarity\n",
        "        batch_features = normalize(batch_features)\n",
        "        features.append(batch_features)\n",
        "\n",
        "    features = np.vstack(features)\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Y114pUA6CN0y"
      },
      "outputs": [],
      "source": [
        "class LwPClassifier:\n",
        "    def __init__(self, num_classes, feature_dim):\n",
        "        \"\"\"\n",
        "        Initializes the LwP classifier with empty prototypes.\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): Number of classes.\n",
        "            feature_dim (int): Dimensionality of feature vectors.\n",
        "        \"\"\"\n",
        "        self.num_classes = num_classes\n",
        "        self.feature_dim = feature_dim\n",
        "        # Initialize prototypes as zero vectors\n",
        "        self.prototypes = np.zeros((num_classes, feature_dim))\n",
        "        # Count of samples per class for prototype updating\n",
        "        self.class_counts = np.zeros(num_classes)\n",
        "\n",
        "    def predict_probabilities(self, features):\n",
        "        \"\"\"\n",
        "        Computes normalized cosine similarity scores as probabilities.\n",
        "\n",
        "        Args:\n",
        "            features (numpy.ndarray): Feature vectors.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Probability-like scores for each class.\n",
        "        \"\"\"\n",
        "        similarities = cosine_similarity(features, self.prototypes)\n",
        "        # Normalize similarities to range [0, 1] to act as probabilities\n",
        "        exp_similarities = np.exp(similarities)  # Exponentiate to ensure positivity\n",
        "        probabilities = exp_similarities / np.sum(exp_similarities, axis=1, keepdims=True)\n",
        "        return probabilities\n",
        "\n",
        "    def initialize_prototypes(self, features, labels):\n",
        "        \"\"\"\n",
        "        Initializes class prototypes based on the initial labeled data.\n",
        "\n",
        "        Args:\n",
        "            features (numpy.ndarray): Feature vectors.\n",
        "            labels (numpy.ndarray): Corresponding labels.\n",
        "        \"\"\"\n",
        "        for class_idx in range(self.num_classes):\n",
        "            class_features = features[labels == class_idx]\n",
        "            if len(class_features) > 0:\n",
        "                self.prototypes[class_idx] = np.mean(class_features, axis=0)\n",
        "                self.prototypes[class_idx] /= np.linalg.norm(self.prototypes[class_idx])  # Normalize\n",
        "                self.class_counts[class_idx] = len(class_features)\n",
        "            else:\n",
        "                # Handle classes with no initial samples\n",
        "                self.prototypes[class_idx] = np.random.randn(self.feature_dim)\n",
        "                self.prototypes[class_idx] /= np.linalg.norm(self.prototypes[class_idx])\n",
        "                self.class_counts[class_idx] = 1\n",
        "\n",
        "    def predict(self, features):\n",
        "        \"\"\"\n",
        "        Predicts class labels for given feature vectors.\n",
        "\n",
        "        Args:\n",
        "            features (numpy.ndarray): Feature vectors.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Predicted class labels.\n",
        "        \"\"\"\n",
        "        similarities = cosine_similarity(features, self.prototypes)\n",
        "        return np.argmax(similarities, axis=1)\n",
        "\n",
        "    def update_prototypes(self, features, labels):\n",
        "        \"\"\"\n",
        "        Updates class prototypes with new labeled data.\n",
        "\n",
        "        Args:\n",
        "            features (numpy.ndarray): Newly labeled feature vectors.\n",
        "            labels (numpy.ndarray): Corresponding labels.\n",
        "        \"\"\"\n",
        "        for class_idx in range(self.num_classes):\n",
        "            class_features = features[labels == class_idx]\n",
        "            if len(class_features) == 0:\n",
        "                continue  # No new data for this class\n",
        "            # Compute mean of new features\n",
        "            new_mean = np.mean(class_features, axis=0)\n",
        "            new_mean /= np.linalg.norm(new_mean)\n",
        "            # Update prototype using running average\n",
        "            total_count = self.class_counts[class_idx] + len(class_features)\n",
        "            self.prototypes[class_idx] = (self.prototypes[class_idx] * self.class_counts[class_idx] + new_mean * len(class_features)) / total_count\n",
        "            self.prototypes[class_idx] /= np.linalg.norm(self.prototypes[class_idx])  # Normalize\n",
        "            self.class_counts[class_idx] = total_count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSwgAIEBDAGc",
        "outputId": "4ef0b302-2a2c-484f-c11a-8af26f7b098b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-14311c9187a9>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  D1 = torch.load('1_train_data.tar.pth')\n",
            "<ipython-input-6-14311c9187a9>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  D2 = torch.load('2_train_data.tar.pth')\n",
            "<ipython-input-6-14311c9187a9>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  D3 = torch.load('3_train_data.tar.pth')\n",
            "<ipython-input-6-14311c9187a9>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  D4 = torch.load('4_train_data.tar.pth')\n",
            "<ipython-input-6-14311c9187a9>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  D5 = torch.load('5_train_data.tar.pth')\n",
            "<ipython-input-6-14311c9187a9>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  D6 = torch.load('6_train_data.tar.pth')\n",
            "<ipython-input-6-14311c9187a9>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  D7 = torch.load('7_train_data.tar.pth')\n",
            "<ipython-input-6-14311c9187a9>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  D8 = torch.load('8_train_data.tar.pth')\n",
            "<ipython-input-6-14311c9187a9>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  D9 = torch.load('9_train_data.tar.pth')\n",
            "<ipython-input-6-14311c9187a9>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  D10 = torch.load('10_train_data.tar.pth')\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "D1 = torch.load('1_train_data.tar.pth')\n",
        "#print(t.keys()) # it will print data and targets\n",
        "#data, targets = t['data'], t['targets'] # both numpy.ndarray\n",
        "D2 = torch.load('2_train_data.tar.pth')\n",
        "D3 = torch.load('3_train_data.tar.pth')\n",
        "D4 = torch.load('4_train_data.tar.pth')\n",
        "D5 = torch.load('5_train_data.tar.pth')\n",
        "D6 = torch.load('6_train_data.tar.pth')\n",
        "D7 = torch.load('7_train_data.tar.pth')\n",
        "D8 = torch.load('8_train_data.tar.pth')\n",
        "D9 = torch.load('9_train_data.tar.pth')\n",
        "D10 = torch.load('10_train_data.tar.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e9AAVkxD_SL",
        "outputId": "ef28b4e0-7bac-4836-b4e2-9ad61630d23b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-520c8c372781>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  T1 = torch.load('1_eval_data.tar.pth')\n",
            "<ipython-input-7-520c8c372781>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  T2 = torch.load('2_eval_data.tar.pth')\n",
            "<ipython-input-7-520c8c372781>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  T3 = torch.load('3_eval_data.tar.pth')\n",
            "<ipython-input-7-520c8c372781>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  T4 = torch.load('4_eval_data.tar.pth')\n",
            "<ipython-input-7-520c8c372781>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  T5 = torch.load('5_eval_data.tar.pth')\n",
            "<ipython-input-7-520c8c372781>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  T6 = torch.load('6_eval_data.tar.pth')\n",
            "<ipython-input-7-520c8c372781>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  T7 = torch.load('7_eval_data.tar.pth')\n",
            "<ipython-input-7-520c8c372781>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  T8 = torch.load('8_eval_data.tar.pth')\n",
            "<ipython-input-7-520c8c372781>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  T9 = torch.load('9_eval_data.tar.pth')\n",
            "<ipython-input-7-520c8c372781>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  T10 = torch.load('10_eval_data.tar.pth')\n"
          ]
        }
      ],
      "source": [
        "T1 = torch.load('1_eval_data.tar.pth')\n",
        "T2 = torch.load('2_eval_data.tar.pth')\n",
        "T3 = torch.load('3_eval_data.tar.pth')\n",
        "T4 = torch.load('4_eval_data.tar.pth')\n",
        "T5 = torch.load('5_eval_data.tar.pth')\n",
        "T6 = torch.load('6_eval_data.tar.pth')\n",
        "T7 = torch.load('7_eval_data.tar.pth')\n",
        "T8 = torch.load('8_eval_data.tar.pth')\n",
        "T9 = torch.load('9_eval_data.tar.pth')\n",
        "T10 = torch.load('10_eval_data.tar.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ao__fDwREpJK"
      },
      "outputs": [],
      "source": [
        "Testsets = []\n",
        "data = T1['data']  # Shape: (num_samples, 32, 32, 3)\n",
        "target = T1['targets']  # Shape: (num_samples,)\n",
        "Testsets.append({'data': data, 'target': target})\n",
        "data = T2['data']\n",
        "target = T2['targets']\n",
        "Testsets.append({'data': data, 'target': target})\n",
        "data = T3['data']\n",
        "target = T3['targets']\n",
        "Testsets.append({'data': data, 'target': target})\n",
        "data = T4['data']\n",
        "target = T4['targets']\n",
        "Testsets.append({'data': data, 'target': target})\n",
        "data = T5['data']\n",
        "target = T5['targets']\n",
        "Testsets.append({'data': data, 'target': target})\n",
        "data = T6['data']\n",
        "target = T6['targets']\n",
        "Testsets.append({'data': data, 'target': target})\n",
        "data = T7['data']\n",
        "target = T7['targets']\n",
        "Testsets.append({'data': data, 'target': target})\n",
        "data = T8['data']\n",
        "target = T8['targets']\n",
        "Testsets.append({'data': data, 'target': target})\n",
        "data = T9['data']\n",
        "target = T9['targets']\n",
        "Testsets.append({'data': data, 'target': target})\n",
        "data = T10['data']\n",
        "target = T10['targets']\n",
        "Testsets.append({'data': data, 'target': target})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cyz6xtzhDdVO"
      },
      "outputs": [],
      "source": [
        "datasets = []\n",
        "data = D1['data']  # Shape: (num_samples, 32, 32, 3)\n",
        "target = D1['targets']  # Shape: (num_samples,)\n",
        "datasets.append({'data': data, 'target': target})\n",
        "\n",
        "data = D2['data']  # Shape: (num_samples, 32, 32,)\n",
        "datasets.append({'data': data})\n",
        "data = D3['data']\n",
        "datasets.append({'data': data})\n",
        "data = D4['data']\n",
        "datasets.append({'data': data})\n",
        "data = D5['data']\n",
        "datasets.append({'data': data})\n",
        "data = D6['data']\n",
        "datasets.append({'data': data})\n",
        "data = D7['data']\n",
        "datasets.append({'data': data})\n",
        "data = D8['data']\n",
        "datasets.append({'data': data})\n",
        "data = D9['data']\n",
        "datasets.append({'data': data})\n",
        "data = D10['data']\n",
        "datasets.append({'data': data})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1dCp3hiiJSNq"
      },
      "outputs": [],
      "source": [
        "for i in range(10):  # Only first 10 datasets as per the problem\n",
        "    #print(f\"Extracting features for D{i+1}...\")\n",
        "    data = datasets[i]['data']\n",
        "    #feature_extractor = initialize_custom_cnn()\n",
        "    features = extract_features(data)\n",
        "    datasets[i]['features'] = features\n",
        "    if i == 0:\n",
        "        labels = datasets[i]['target']\n",
        "        datasets[i]['labels'] = labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wIhJuGgnF0zl"
      },
      "outputs": [],
      "source": [
        "for i in range(10):  # Only first 10 datasets as per the problem\n",
        "    #print(f\"Extracting features for D{i+1}...\")\n",
        "    data = Testsets[i]['data']\n",
        "    #feature_extractor = initialize_feature_extractor()\n",
        "    #features = extract_features(data,batch_size=128)\n",
        "    #feature_extractor = initialize_custom_cnn()\n",
        "    features = extract_features(data)\n",
        "    #features = extract_features(data)\n",
        "    Testsets[i]['features'] = features\n",
        "    labels = Testsets[i]['target']\n",
        "    Testsets[i]['labels'] = labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RMhqxFwfD7i_"
      },
      "outputs": [],
      "source": [
        "num_classes = 10  # CIFAR-10 has 10 classes\n",
        "feature_dim = 1280  # ResNet50 with pooling='avg' outputs 2048-dimensional vectors\n",
        "\n",
        "# Initialize LwP classifier\n",
        "lwpc = LwPClassifier(num_classes=num_classes, feature_dim=feature_dim)\n",
        "#lwpc = SigmoidKernelLwPClassifier(num_classes=10, feature_dim=1280, alpha=1.0, c=0.05)\n",
        "\n",
        "# Initialize prototypes using D1\n",
        "lwpc.initialize_prototypes(datasets[0]['features'], datasets[0]['labels'])\n",
        "\n",
        "models = []\n",
        "models.append(lwpc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2wlvP-hLavI",
        "outputId": "31cfb1c8-b645-4c83-ca60-405e4048cd1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#########################\n",
            "testing for 1th model\n",
            "#########################\n",
            "Accuracy on T1: 0.7776\n",
            "#########################\n",
            "testing for 2th model\n",
            "#########################\n",
            "Accuracy on T1: 0.7644\n",
            "Accuracy on T2: 0.7608\n",
            "#########################\n",
            "testing for 3th model\n",
            "#########################\n",
            "Accuracy on T1: 0.7604\n",
            "Accuracy on T2: 0.7572\n",
            "Accuracy on T3: 0.7444\n",
            "#########################\n",
            "testing for 4th model\n",
            "#########################\n",
            "Accuracy on T1: 0.7552\n",
            "Accuracy on T2: 0.7568\n",
            "Accuracy on T3: 0.7408\n",
            "Accuracy on T4: 0.754\n",
            "#########################\n",
            "testing for 5th model\n",
            "#########################\n",
            "Accuracy on T1: 0.7536\n",
            "Accuracy on T2: 0.7544\n",
            "Accuracy on T3: 0.7412\n",
            "Accuracy on T4: 0.752\n",
            "Accuracy on T5: 0.7688\n",
            "#########################\n",
            "testing for 6th model\n",
            "#########################\n",
            "Accuracy on T1: 0.7536\n",
            "Accuracy on T2: 0.752\n",
            "Accuracy on T3: 0.7384\n",
            "Accuracy on T4: 0.7492\n",
            "Accuracy on T5: 0.7672\n",
            "Accuracy on T6: 0.7652\n",
            "#########################\n",
            "testing for 7th model\n",
            "#########################\n",
            "Accuracy on T1: 0.7532\n",
            "Accuracy on T2: 0.7528\n",
            "Accuracy on T3: 0.7352\n",
            "Accuracy on T4: 0.7504\n",
            "Accuracy on T5: 0.7664\n",
            "Accuracy on T6: 0.7664\n",
            "Accuracy on T7: 0.736\n",
            "#########################\n",
            "testing for 8th model\n",
            "#########################\n",
            "Accuracy on T1: 0.7512\n",
            "Accuracy on T2: 0.7488\n",
            "Accuracy on T3: 0.7344\n",
            "Accuracy on T4: 0.75\n",
            "Accuracy on T5: 0.7648\n",
            "Accuracy on T6: 0.7668\n",
            "Accuracy on T7: 0.7352\n",
            "Accuracy on T8: 0.746\n",
            "#########################\n",
            "testing for 9th model\n",
            "#########################\n",
            "Accuracy on T1: 0.7508\n",
            "Accuracy on T2: 0.748\n",
            "Accuracy on T3: 0.7328\n",
            "Accuracy on T4: 0.7496\n",
            "Accuracy on T5: 0.7644\n",
            "Accuracy on T6: 0.7652\n",
            "Accuracy on T7: 0.7332\n",
            "Accuracy on T8: 0.7456\n",
            "Accuracy on T9: 0.7404\n",
            "#########################\n",
            "testing for 10th model\n",
            "#########################\n",
            "Accuracy on T1: 0.7504\n",
            "Accuracy on T2: 0.7492\n",
            "Accuracy on T3: 0.7324\n",
            "Accuracy on T4: 0.7492\n",
            "Accuracy on T5: 0.7632\n",
            "Accuracy on T6: 0.764\n",
            "Accuracy on T7: 0.7336\n",
            "Accuracy on T8: 0.7464\n",
            "Accuracy on T9: 0.7404\n",
            "Accuracy on T10: 0.7524\n"
          ]
        }
      ],
      "source": [
        "# Initialize prototypes using D1\n",
        "#lwpc = LwPClassifier(num_classes=num_classes, feature_dim=feature_dim)\n",
        "#lwpc.initialize_prototypes(datasets[0]['features'], datasets[0]['labels'])\n",
        "\n",
        "for i in range(0, 10):  # Indices 1 to 9 correspond to D2 to D10\n",
        "    print(\"#########################\")\n",
        "    print(f\"testing for {i+1}th model\")\n",
        "    print(\"#########################\")\n",
        "\n",
        "    if i!=0 :\n",
        "        current_features = datasets[i]['features']\n",
        "        predicted_labels = lwpc.predict(current_features)\n",
        "        datasets[i]['predicted_labels'] = predicted_labels\n",
        "        lwpc.update_prototypes(current_features, predicted_labels)\n",
        "        models.append(lwpc)\n",
        "\n",
        "    for j in range(0,i+1):\n",
        "           feature = Testsets[j]['features']\n",
        "           label = Testsets[j]['labels']\n",
        "\n",
        "           predictions = lwpc.predict(feature)\n",
        "           accuracy = np.mean(predictions == label)\n",
        "           print(f\"Accuracy on T{j+1}: {accuracy}\")"
      ]
    }
  ]
}